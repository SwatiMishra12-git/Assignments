{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPz9TSRMrmbJew64JiHLPgj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8HCS-3vOt1R"
      },
      "outputs": [],
      "source": [
        "#Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The F-distribution is a continuous probability distribution that arises frequently in the field of statistics, especially in analysis of variance (ANOVA) and regression analysis. Here are some key properties of the F-distribution:\n",
        "\n",
        "1. **Non-Negative Values**: The F-distribution is defined only for non-negative values. This means the distribution only takes values greater than or equal to zero.\n",
        "\n",
        "2. **Asymmetry**: The F-distribution is skewed to the right, meaning it has a long tail on the right side. However, the degree of skewness decreases as the degrees of freedom increase.\n",
        "\n",
        "3. **Degrees of Freedom**: The shape of the F-distribution depends on two parameters, often denoted as \\(d_1\\) and \\(d_2\\), which are the degrees of freedom for the numerator and denominator, respectively. These parameters control the shape and spread of the distribution.\n",
        "\n",
        "4. **Mean and Variance**:\n",
        "   - The mean of the F-distribution is \\(\\frac{d_2}{d_2 - 2}\\), provided that \\(d_2 > 2\\).\n",
        "   - The variance is \\(\\frac{2d_2^2(d_1 + d_2 - 2)}{d_1(d_2 - 2)^2(d_2 - 4)}\\), provided that \\(d_2 > 4\\).\n",
        "\n",
        "5. **Uses in Hypothesis Testing**: The F-distribution is primarily used to compare two sample variances and is utilized in the F-test. It is also used in ANOVA to test the hypothesis that several population means are equal.\n",
        "\n",
        "6. **Distribution Family**: The F-distribution is part of the family of ratio distributions, specifically the ratio of two scaled chi-squared distributions.\n",
        "\n",
        "Here is a graph to visualize the F-distribution with different degrees of freedom for numerator and denominator (F_{d_1, d_2}):\n",
        "\n",
        "$$\n",
        "F_{d_1, d_2} = \\frac{\\chi^2_{d_1}/d_1}{\\chi^2_{d_2}/d_2}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "eURY-z5XP2AG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 2"
      ],
      "metadata": {
        "id": "m6fyJHO6O9Vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The F-distribution is commonly used in several types of statistical tests, particularly in scenarios where we compare variances or test the relationship between variables. Here are some key tests where the F-distribution plays a crucial role:\n",
        "\n",
        "1. **Analysis of Variance (ANOVA)**:\n",
        "   - **Purpose**: Used to compare the means of three or more groups to see if at least one of the group means is significantly different from the others.\n",
        "   - **Why F-distribution is used**: The F-test in ANOVA compares the variance between the groups to the variance within the groups. The F-distribution is appropriate because it provides a way to determine if the observed variances are significantly different, considering the sample size and degrees of freedom.\n",
        "\n",
        "2. **Regression Analysis**:\n",
        "   - **Purpose**: Used to assess the relationship between dependent and independent variables.\n",
        "   - **Why F-distribution is used**: In multiple regression analysis, the F-test is used to evaluate whether the overall regression model is a good fit for the data. It compares the model with and without the independent variables to see if adding the variables significantly improves the model.\n",
        "\n",
        "3. **F-Test for Equality of Variances**:\n",
        "   - **Purpose**: Used to compare the variances of two populations to see if they are equal.\n",
        "   - **Why F-distribution is used**: The test statistic follows an F-distribution when the data are normally distributed. The F-test provides a way to determine if the observed variance ratio is significantly different from what we would expect under the null hypothesis (equal variances).\n",
        "\n",
        "4. **MANOVA (Multivariate Analysis of Variance)**:\n",
        "   - **Purpose**: An extension of ANOVA that allows for the comparison of multiple dependent variables across groups.\n",
        "   - **Why F-distribution is used**: The F-test in MANOVA evaluates whether the means of multiple dependent variables are different across groups. The test statistic follows an F-distribution, making it suitable for this multivariate scenario.\n",
        "\n",
        "5. **General Linear Models (GLM)**:\n",
        "   - **Purpose**: Used to model the relationship between multiple predictors and a dependent variable.\n",
        "   - **Why F-distribution is used**: In the context of GLMs, the F-test assesses the significance of predictors in the model. The test statistic follows an F-distribution, helping to determine if the predictors are significantly related to the dependent variable.\n",
        "\n",
        "The F-distribution is particularly useful in these tests because it accounts for the degrees of freedom in both the numerator (between-group variance) and the denominator (within-group variance). This allows it to appropriately evaluate the significance of observed differences in variances or model fits.\n"
      ],
      "metadata": {
        "id": "-LEhBOvgRO9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 3"
      ],
      "metadata": {
        "id": "FA8Bxh2TO9Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When conducting an F-test to compare the variances of two populations, there are several key assumptions that need to be met to ensure the validity of the test results:\n",
        "\n",
        "1. **Independence**: The samples drawn from the two populations must be independent of each other. This means the selection of one sample does not influence the selection of the other sample.\n",
        "\n",
        "2. **Normality**: The data in both populations should follow a normal distribution. The F-test is sensitive to departures from normality, and non-normal data can lead to incorrect conclusions.\n",
        "\n",
        "3. **Scale of Measurement**: The data should be measured on at least an interval scale. This ensures that the differences between data points are meaningful and consistent.\n",
        "\n",
        "4. **Ratio of Variances**: The ratio of the variances of the two populations should be equal under the null hypothesis. This forms the basis of the F-test, which compares the observed variance ratio to what is expected under the null hypothesis.\n",
        "\n",
        "5. **Random Sampling**: The data should be collected using a random sampling method to ensure that the samples are representative of the populations.\n",
        "\n",
        "6. **Homogeneity of Variances**: Although the F-test is used to compare variances, it assumes that the variances of the two populations are equal under the null hypothesis. This is why the test is designed to detect differences in variances.\n",
        "\n",
        "7. **Sample Size**: The sample sizes from both populations should be large enough to provide reliable estimates of the variances. However, the F-test can be sensitive to differences in sample sizes, so equal or similar sample sizes are preferred.\n",
        "\n",
        "By ensuring these assumptions are met, you can be more confident that the results of the F-test will be valid and reliable. If these assumptions are violated, the results of the test may be misleading, and alternative statistical methods should be considered.\n",
        "\n"
      ],
      "metadata": {
        "id": "bbUFwYozRa3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 4"
      ],
      "metadata": {
        "id": "IKTs07qEO8-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great question! Both ANOVA and t-tests are used to compare means, but they serve different purposes and are used in different contexts.\n",
        "\n",
        "**Purpose of ANOVA (Analysis of Variance)**:\n",
        "- **Comparison of Multiple Groups**: The main purpose of ANOVA is to compare the means of three or more groups to determine if at least one of the group means is significantly different from the others.\n",
        "- **Variation Analysis**: ANOVA decomposes the total variation in the data into variation between groups and within groups. It helps in identifying if the between-group variation is larger than the within-group variation.\n",
        "- **F-Test**: ANOVA uses the F-test to determine statistical significance. It compares the ratio of between-group variance to within-group variance, which follows an F-distribution.\n",
        "\n",
        "**Difference from a t-test**:\n",
        "- **Number of Groups**: A t-test is typically used to compare the means of two groups (independent or paired samples). In contrast, ANOVA is used when there are three or more groups to compare.\n",
        "- **Test Statistic**: The t-test uses the t-distribution to determine statistical significance, while ANOVA uses the F-distribution.\n",
        "- **Types of Comparisons**:\n",
        "  - **Independent t-test**: Compares the means of two independent groups.\n",
        "  - **Paired t-test**: Compares the means of two related groups (e.g., before and after measurements).\n",
        "  - **One-Way ANOVA**: Compares the means of three or more independent groups based on one factor.\n",
        "  - **Two-Way ANOVA**: Compares the means of groups based on two factors, and can also assess interaction effects between the factors.\n",
        "\n",
        "**Example**:\n",
        "- **t-test**: You have two sets of test scores from two different classes, and you want to compare the average scores of the two classes.\n",
        "- **ANOVA**: You have test scores from three or more classes, and you want to see if there is a significant difference in the average scores among all the classes.\n"
      ],
      "metadata": {
        "id": "FOTOg7E7Rwgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 5"
      ],
      "metadata": {
        "id": "pHydDbnEO8yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When comparing more than two groups, using a one-way ANOVA instead of multiple t-tests is generally preferred for several important reasons:\n",
        "\n",
        "1. **Control of Type I Error Rate**:\n",
        "   - When performing multiple t-tests, each test has its own probability of making a Type I error (false positive). If you conduct multiple tests, the cumulative probability of making at least one Type I error increases.\n",
        "   - One-way ANOVA controls the overall Type I error rate by performing a single test to compare all group means simultaneously. This maintains the significance level (e.g., α = 0.05) for the entire set of comparisons.\n",
        "\n",
        "2. **Efficiency**:\n",
        "   - One-way ANOVA is a more efficient way to test for differences among several group means because it consolidates the analysis into a single test. This reduces the complexity of the analysis and makes it easier to interpret the results.\n",
        "\n",
        "3. **Multiple Comparisons**:\n",
        "   - If the one-way ANOVA indicates that there are significant differences among the group means, post-hoc tests (such as Tukey's HSD) can be performed to determine which specific pairs of groups are different. This approach still maintains control over the Type I error rate.\n",
        "\n",
        "4. **Variance Analysis**:\n",
        "   - One-way ANOVA allows for the analysis of variance within and between groups. It helps in understanding the sources of variation in the data, which is not possible with multiple t-tests.\n",
        "\n",
        "**Example**:\n",
        "Suppose you have test scores from three different classes (Group A, Group B, and Group C), and you want to determine if there is a significant difference in the average scores among these classes.\n",
        "\n",
        "- **Using multiple t-tests**: You would need to perform three pairwise comparisons (A vs. B, A vs. C, and B vs. C). Each comparison has its own risk of a Type I error, which accumulates across the tests.\n",
        "- **Using one-way ANOVA**: You perform a single test to compare the means of all three groups simultaneously. If the ANOVA result is significant, you can then perform post-hoc tests to identify which specific pairs of groups differ.\n",
        "\n"
      ],
      "metadata": {
        "id": "61I4bo8_R70z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 6"
      ],
      "metadata": {
        "id": "vuRdQjuhO8no"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " In ANOVA, the total variance observed in the data is partitioned into two components: between-group variance and within-group variance. This partitioning helps in understanding the sources of variation and contributes to the calculation of the F-statistic, which is used to determine if the group means are significantly different.\n",
        "\n",
        "**1. Total Variance (SS_T)**:\n",
        "- Total variance represents the overall variability in the data, regardless of the group memberships.\n",
        "- It is calculated as the sum of the squared differences between each individual observation and the overall mean of all observations.\n",
        "\\[ \\text{SS}_{\\text{T}} = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (X_{ij} - \\bar{X}_{\\text{overall}})^2 \\]\n",
        "  - \\(k\\): Number of groups.\n",
        "  - \\(n_i\\): Number of observations in the \\(i\\)-th group.\n",
        "  - \\(X_{ij}\\): \\(j\\)-th observation in the \\(i\\)-th group.\n",
        "  - \\(\\bar{X}_{\\text{overall}}\\): Overall mean of all observations.\n",
        "\n",
        "**2. Between-Group Variance (SS_B)**:\n",
        "- Between-group variance measures the variability due to the differences between the group means.\n",
        "- It is calculated as the sum of the squared differences between the group means and the overall mean, weighted by the number of observations in each group.\n",
        "\\[ \\text{SS}_{\\text{B}} = \\sum_{i=1}^{k} n_i (\\bar{X}_i - \\bar{X}_{\\text{overall}})^2 \\]\n",
        "  - \\(\\bar{X}_i\\): Mean of the \\(i\\)-th group.\n",
        "\n",
        "**3. Within-Group Variance (SS_W)**:\n",
        "- Within-group variance measures the variability within each group, due to the individual differences among observations within the same group.\n",
        "- It is calculated as the sum of the squared differences between each individual observation and its corresponding group mean.\n",
        "\\[ \\text{SS}_{\\text{W}} = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (X_{ij} - \\bar{X}_i)^2 \\]\n",
        "\n",
        "**Partitioning**:\n",
        "\\[ \\text{SS}_{\\text{T}} = \\text{SS}_{\\text{B}} + \\text{SS}_{\\text{W}} \\]\n",
        "\n",
        "**Calculation of the F-statistic**:\n",
        "The F-statistic is used to determine if the between-group variance is significantly larger than the within-group variance. It is calculated as the ratio of the mean square between groups (MS_B) to the mean square within groups (MS_W).\n",
        "\n",
        "1. **Mean Square Between Groups (MS_B)**:\n",
        "\\[ \\text{MS}_{\\text{B}} = \\frac{\\text{SS}_{\\text{B}}}{k - 1} \\]\n",
        "\n",
        "2. **Mean Square Within Groups (MS_W)**:\n",
        "\\[ \\text{MS}_{\\text{W}} = \\frac{\\text{SS}_{\\text{W}}}{N - k} \\]\n",
        "  - \\(N\\): Total number of observations.\n",
        "\n",
        "3. **F-statistic**:\n",
        "\\[ F = \\frac{\\text{MS}_{\\text{B}}}{\\text{MS}_{\\text{W}}} \\]\n",
        "\n",
        "The F-statistic follows an F-distribution with \\(k - 1\\) and \\(N - k\\) degrees of freedom. A large F-statistic indicates that the between-group variance is significantly greater than the within-group variance, suggesting that at least one group mean is different from the others.\n",
        "\n"
      ],
      "metadata": {
        "id": "HU75lf_TSesA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7"
      ],
      "metadata": {
        "id": "z1dnGUmIO8bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the classical (frequentist) approach to ANOVA with the Bayesian approach. These two frameworks differ fundamentally in how they handle uncertainty, parameter estimation, and hypothesis testing.\n",
        "\n",
        "**1. Handling Uncertainty**:\n",
        "\n",
        "- **Frequentist Approach**:\n",
        "  - In the frequentist approach, uncertainty is handled through the use of sampling distributions and confidence intervals. Uncertainty is measured by the probability of observing the data given the null hypothesis.\n",
        "  - Confidence intervals provide a range of values within which the true parameter is expected to lie, with a certain level of confidence (e.g., 95%).\n",
        "\n",
        "- **Bayesian Approach**:\n",
        "  - In the Bayesian approach, uncertainty is handled through probability distributions. Bayesian inference combines prior information about the parameters with the observed data to produce posterior distributions.\n",
        "  - Credible intervals are used to represent the range of parameter values with a certain level of probability (e.g., 95%), reflecting the degree of belief in the parameters given the data and prior information.\n",
        "\n",
        "**2. Parameter Estimation**:\n",
        "\n",
        "- **Frequentist Approach**:\n",
        "  - Parameters are estimated using point estimates, such as the sample mean or variance, and these estimates are treated as fixed values.\n",
        "  - Maximum likelihood estimation (MLE) is often used to find the parameter values that maximize the likelihood of observing the data.\n",
        "\n",
        "- **Bayesian Approach**:\n",
        "  - Parameters are treated as random variables with probability distributions. Bayesian estimation provides a posterior distribution for each parameter, reflecting the uncertainty about its true value.\n",
        "  - Prior distributions represent the initial beliefs about the parameters before observing the data. These are updated with the data to obtain the posterior distributions using Bayes' theorem.\n",
        "\n",
        "**3. Hypothesis Testing**:\n",
        "\n",
        "- **Frequentist Approach**:\n",
        "  - Hypothesis testing is based on p-values and significance levels (e.g., α = 0.05). The null hypothesis is rejected if the p-value is less than the significance level.\n",
        "  - ANOVA uses the F-statistic to test the null hypothesis that all group means are equal. If the F-statistic exceeds a critical value from the F-distribution, the null hypothesis is rejected.\n",
        "\n",
        "- **Bayesian Approach**:\n",
        "  - Hypothesis testing is based on posterior probabilities and Bayes factors. Bayesian analysis directly quantifies the evidence in favor of or against a hypothesis.\n",
        "  - The Bayes factor compares the likelihood of the data under two competing hypotheses (e.g., null and alternative). A higher Bayes factor indicates stronger evidence for one hypothesis over the other.\n",
        "\n",
        "**Key Differences**:\n",
        "\n",
        "- **Frequentist**:\n",
        "  - Relies on long-run frequency properties and fixed parameters.\n",
        "  - Uses confidence intervals, p-values, and significance levels for inference.\n",
        "  - Based on the likelihood of observing the data under the null hypothesis.\n",
        "\n",
        "- **Bayesian**:\n",
        "  - Relies on probability distributions and treats parameters as random variables.\n",
        "  - Uses prior and posterior distributions, credible intervals, and Bayes factors for inference.\n",
        "  - Based on updating beliefs about parameters with observed data.\n",
        "\n",
        "Both approaches have their strengths and weaknesses, and the choice between them depends on the context of the analysis, the availability of prior information, and the research goals."
      ],
      "metadata": {
        "id": "VIclaz8RSufM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8"
      ],
      "metadata": {
        "id": "lYtHa81XO8PP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import f\n",
        "\n",
        "# Data for Profession A and Profession B\n",
        "profession_a = np.array([48, 52, 55, 60, 62])\n",
        "profession_b = np.array([45, 50, 55, 52, 47])\n",
        "\n",
        "# Calculate variances\n",
        "variance_a = np.var(profession_a, ddof=1)\n",
        "variance_b = np.var(profession_b, ddof=1)\n",
        "\n",
        "# Calculate the F-statistic\n",
        "f_statistic = variance_a / variance_b\n",
        "\n",
        "# Calculate the degrees of freedom\n",
        "dfn = len(profession_a) - 1  # degrees of freedom for numerator\n",
        "dfd = len(profession_b) - 1  # degrees of freedom for denominator\n",
        "\n",
        "# Calculate the p-value using the F-distribution\n",
        "p_value = 1 - f.cdf(f_statistic, dfn, dfd)\n",
        "\n",
        "# Output results\n",
        "print(f\"Variance of Profession A: {variance_a}\")\n",
        "print(f\"Variance of Profession B: {variance_b}\")\n",
        "print(f\"F-Statistic: {f_statistic}\")\n",
        "print(f\"P-Value: {p_value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmUBSfvnTejg",
        "outputId": "383dcc73-c175-4ebd-c097-4bb01383f23b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variance of Profession A: 32.8\n",
            "Variance of Profession B: 15.7\n",
            "F-Statistic: 2.089171974522293\n",
            "P-Value: 0.24652429950266952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation:\n",
        "\n",
        "F-Statistic: The F-statistic is 2.089. This is the ratio of the variances of Profession A to Profession B.\n",
        "\n",
        "P-Value: The p-value is approximately 0.246.\n",
        "\n",
        "Conclusion: Since the p-value (0.246) is greater than the common significance level (e.g., α = 0.05), we fail to reject the null hypothesis. This indicates that there is not enough evidence to conclude that the variances of the incomes of Profession A and Profession B are significantly different."
      ],
      "metadata": {
        "id": "8ic02xgyTwgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9"
      ],
      "metadata": {
        "id": "poSjYwVGO8CU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Data for three regions\n",
        "region_a = np.array([160, 162, 165, 158, 164])\n",
        "region_b = np.array([172, 175, 170, 168, 174])\n",
        "region_c = np.array([180, 182, 179, 185, 183])\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_statistic, p_value = f_oneway(region_a, region_b, region_c)\n",
        "\n",
        "# Output results\n",
        "print(f\"F-Statistic: {f_statistic}\")\n",
        "print(f\"P-Value: {p_value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9r36YBOUJh4",
        "outputId": "26087537-07e6-4e47-d112-1e5c7527bf53"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-Statistic: 67.87330316742101\n",
            "P-Value: 2.870664187937026e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Since the p-value  is much smaller than the common significance level (e.g., α = 0.05), we reject the null hypothesis. This means there is strong evidence to conclude that there are statistically significant differences in the average heights between the three regions (Region A, Region B, and Region C).\n",
        "\n",
        "In other words, the one-way ANOVA results indicate that the average heights of individuals in the three regions are not the same. There is at least one region with a significantly different average height compared to the others."
      ],
      "metadata": {
        "id": "_Qc8A6SbUSk1"
      }
    }
  ]
}